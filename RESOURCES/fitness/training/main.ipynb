{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df919ef",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04741235",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "04741235",
    "outputId": "018c6bdb-a1e2-4edb-ab0b-51bf08e5eb28"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e2aaaab-a8ca-46fb-a18f-02c57498dd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5840987-11d6-4a34-a6d4-07f978f6f28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yU_1</th>\n",
       "      <th>yU_2</th>\n",
       "      <th>yU_3</th>\n",
       "      <th>yU_4</th>\n",
       "      <th>yU_5</th>\n",
       "      <th>yU_6</th>\n",
       "      <th>yU_7</th>\n",
       "      <th>yU_8</th>\n",
       "      <th>yU_9</th>\n",
       "      <th>yU_10</th>\n",
       "      <th>...</th>\n",
       "      <th>yL_12</th>\n",
       "      <th>yL_13</th>\n",
       "      <th>yL_14</th>\n",
       "      <th>yL_15</th>\n",
       "      <th>ReynoldsNumber</th>\n",
       "      <th>MachNumber</th>\n",
       "      <th>alpha</th>\n",
       "      <th>Cl</th>\n",
       "      <th>Cd</th>\n",
       "      <th>Cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.023923</td>\n",
       "      <td>0.021796</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>0.015248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-10</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.16140</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.023923</td>\n",
       "      <td>0.021796</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>0.015248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-9</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>0.13236</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.023923</td>\n",
       "      <td>0.021796</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>0.015248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-8</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.10163</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.023923</td>\n",
       "      <td>0.021796</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>0.015248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-7</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0.07583</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.023923</td>\n",
       "      <td>0.021796</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>0.015248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-6</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>0.05855</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253189</th>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.009635</td>\n",
       "      <td>-0.014932</td>\n",
       "      <td>-0.018353</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>-0.015217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013828</td>\n",
       "      <td>-0.006325</td>\n",
       "      <td>-0.001614</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.06955</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253190</th>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.009635</td>\n",
       "      <td>-0.014932</td>\n",
       "      <td>-0.018353</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>-0.015217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013828</td>\n",
       "      <td>-0.006325</td>\n",
       "      <td>-0.001614</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253191</th>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.009635</td>\n",
       "      <td>-0.014932</td>\n",
       "      <td>-0.018353</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>-0.015217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013828</td>\n",
       "      <td>-0.006325</td>\n",
       "      <td>-0.001614</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.06682</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253192</th>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.009635</td>\n",
       "      <td>-0.014932</td>\n",
       "      <td>-0.018353</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>-0.015217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013828</td>\n",
       "      <td>-0.006325</td>\n",
       "      <td>-0.001614</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.08164</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253193</th>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.009635</td>\n",
       "      <td>-0.014932</td>\n",
       "      <td>-0.018353</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>-0.015217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013828</td>\n",
       "      <td>-0.006325</td>\n",
       "      <td>-0.001614</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.07665</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253194 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            yU_1      yU_2      yU_3      yU_4      yU_5      yU_6      yU_7  \\\n",
       "0       0.006923  0.013103  0.018233  0.022013  0.024229  0.024823  0.023923   \n",
       "1       0.006923  0.013103  0.018233  0.022013  0.024229  0.024823  0.023923   \n",
       "2       0.006923  0.013103  0.018233  0.022013  0.024229  0.024823  0.023923   \n",
       "3       0.006923  0.013103  0.018233  0.022013  0.024229  0.024823  0.023923   \n",
       "4       0.006923  0.013103  0.018233  0.022013  0.024229  0.024823  0.023923   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "253189  0.004474  0.005027  0.001877 -0.003540 -0.009635 -0.014932 -0.018353   \n",
       "253190  0.004474  0.005027  0.001877 -0.003540 -0.009635 -0.014932 -0.018353   \n",
       "253191  0.004474  0.005027  0.001877 -0.003540 -0.009635 -0.014932 -0.018353   \n",
       "253192  0.004474  0.005027  0.001877 -0.003540 -0.009635 -0.014932 -0.018353   \n",
       "253193  0.004474  0.005027  0.001877 -0.003540 -0.009635 -0.014932 -0.018353   \n",
       "\n",
       "            yU_8      yU_9     yU_10  ...     yL_12     yL_13     yL_14  \\\n",
       "0       0.021796  0.018781  0.015248  ... -0.007926 -0.004725 -0.002193   \n",
       "1       0.021796  0.018781  0.015248  ... -0.007926 -0.004725 -0.002193   \n",
       "2       0.021796  0.018781  0.015248  ... -0.007926 -0.004725 -0.002193   \n",
       "3       0.021796  0.018781  0.015248  ... -0.007926 -0.004725 -0.002193   \n",
       "4       0.021796  0.018781  0.015248  ... -0.007926 -0.004725 -0.002193   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "253189 -0.019393 -0.018142 -0.015217  ... -0.023546 -0.013828 -0.006325   \n",
       "253190 -0.019393 -0.018142 -0.015217  ... -0.023546 -0.013828 -0.006325   \n",
       "253191 -0.019393 -0.018142 -0.015217  ... -0.023546 -0.013828 -0.006325   \n",
       "253192 -0.019393 -0.018142 -0.015217  ... -0.023546 -0.013828 -0.006325   \n",
       "253193 -0.019393 -0.018142 -0.015217  ... -0.023546 -0.013828 -0.006325   \n",
       "\n",
       "           yL_15  ReynoldsNumber  MachNumber  alpha     Cl       Cd     Cm  \n",
       "0      -0.000563          100000         0.1    -10 -0.334  0.16140  0.001  \n",
       "1      -0.000563          100000         0.1     -9 -0.392  0.13236  0.001  \n",
       "2      -0.000563          100000         0.1     -8 -0.442  0.10163  0.001  \n",
       "3      -0.000563          100000         0.1     -7 -0.469  0.07583  0.001  \n",
       "4      -0.000563          100000         0.1     -6 -0.459  0.05855  0.001  \n",
       "...          ...             ...         ...    ...    ...      ...    ...  \n",
       "253189 -0.001614          500000         0.3      6  0.103  0.06955  0.031  \n",
       "253190 -0.001614          500000         0.3      7  0.033  0.05742  0.033  \n",
       "253191 -0.001614          500000         0.3      8  0.044  0.06682  0.034  \n",
       "253192 -0.001614          500000         0.3      9  0.055  0.08164  0.035  \n",
       "253193 -0.001614          500000         0.3     10  0.066  0.07665  0.036  \n",
       "\n",
       "[253194 rows x 36 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d3f8ea3",
   "metadata": {
    "id": "4d3f8ea3"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def assign_airfoil_ids(df):\n",
    "    '''\n",
    "    Takes in a dataframe and returns the dataframe with airfoil ids\n",
    "    '''\n",
    "    # Extract y-coordinate columns\n",
    "    y_columns = df.columns[df.columns.str.startswith('y')]\n",
    "\n",
    "    # Create a new column with a unique identifier for each airfoil\n",
    "    df['Airfoil_No'] = pd.factorize(df[y_columns].apply(tuple, axis=1))[0] # using the factorize function from pandas to convert the tuples into integer IDs.\n",
    "\n",
    "    df = pd.concat([df[['Airfoil_No']], df.drop(columns=['Airfoil_No'])], axis=1)\n",
    "    \n",
    "    # Count the number of unique airfoils\n",
    "    num_airfoils = df['Airfoil_No'].nunique()\n",
    "\n",
    "    print(f\"Number of airfoils: {num_airfoils}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b126112",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b126112",
    "outputId": "3dc1ffd0-3c74-4eae-f2b0-1e8d5a1aa7bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of airfoils: 831\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airfoil_No</th>\n",
       "      <th>yU_1</th>\n",
       "      <th>yU_2</th>\n",
       "      <th>yU_3</th>\n",
       "      <th>yU_4</th>\n",
       "      <th>yU_5</th>\n",
       "      <th>yU_6</th>\n",
       "      <th>yU_7</th>\n",
       "      <th>yU_8</th>\n",
       "      <th>yU_9</th>\n",
       "      <th>...</th>\n",
       "      <th>yL_12</th>\n",
       "      <th>yL_13</th>\n",
       "      <th>yL_14</th>\n",
       "      <th>yL_15</th>\n",
       "      <th>ReynoldsNumber</th>\n",
       "      <th>MachNumber</th>\n",
       "      <th>alpha</th>\n",
       "      <th>Cl</th>\n",
       "      <th>Cd</th>\n",
       "      <th>Cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.023923</td>\n",
       "      <td>0.021796</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-10</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.16140</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.023923</td>\n",
       "      <td>0.021796</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-9</td>\n",
       "      <td>-0.392</td>\n",
       "      <td>0.13236</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.023923</td>\n",
       "      <td>0.021796</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-8</td>\n",
       "      <td>-0.442</td>\n",
       "      <td>0.10163</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.023923</td>\n",
       "      <td>0.021796</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-7</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0.07583</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.006923</td>\n",
       "      <td>0.013103</td>\n",
       "      <td>0.018233</td>\n",
       "      <td>0.022013</td>\n",
       "      <td>0.024229</td>\n",
       "      <td>0.024823</td>\n",
       "      <td>0.023923</td>\n",
       "      <td>0.021796</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007926</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-6</td>\n",
       "      <td>-0.459</td>\n",
       "      <td>0.05855</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253189</th>\n",
       "      <td>830</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.009635</td>\n",
       "      <td>-0.014932</td>\n",
       "      <td>-0.018353</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013828</td>\n",
       "      <td>-0.006325</td>\n",
       "      <td>-0.001614</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.06955</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253190</th>\n",
       "      <td>830</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.009635</td>\n",
       "      <td>-0.014932</td>\n",
       "      <td>-0.018353</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013828</td>\n",
       "      <td>-0.006325</td>\n",
       "      <td>-0.001614</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253191</th>\n",
       "      <td>830</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.009635</td>\n",
       "      <td>-0.014932</td>\n",
       "      <td>-0.018353</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013828</td>\n",
       "      <td>-0.006325</td>\n",
       "      <td>-0.001614</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.06682</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253192</th>\n",
       "      <td>830</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.009635</td>\n",
       "      <td>-0.014932</td>\n",
       "      <td>-0.018353</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013828</td>\n",
       "      <td>-0.006325</td>\n",
       "      <td>-0.001614</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.08164</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253193</th>\n",
       "      <td>830</td>\n",
       "      <td>0.004474</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>-0.009635</td>\n",
       "      <td>-0.014932</td>\n",
       "      <td>-0.018353</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023546</td>\n",
       "      <td>-0.013828</td>\n",
       "      <td>-0.006325</td>\n",
       "      <td>-0.001614</td>\n",
       "      <td>500000</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.07665</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253194 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Airfoil_No      yU_1      yU_2      yU_3      yU_4      yU_5  \\\n",
       "0                0  0.006923  0.013103  0.018233  0.022013  0.024229   \n",
       "1                0  0.006923  0.013103  0.018233  0.022013  0.024229   \n",
       "2                0  0.006923  0.013103  0.018233  0.022013  0.024229   \n",
       "3                0  0.006923  0.013103  0.018233  0.022013  0.024229   \n",
       "4                0  0.006923  0.013103  0.018233  0.022013  0.024229   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "253189         830  0.004474  0.005027  0.001877 -0.003540 -0.009635   \n",
       "253190         830  0.004474  0.005027  0.001877 -0.003540 -0.009635   \n",
       "253191         830  0.004474  0.005027  0.001877 -0.003540 -0.009635   \n",
       "253192         830  0.004474  0.005027  0.001877 -0.003540 -0.009635   \n",
       "253193         830  0.004474  0.005027  0.001877 -0.003540 -0.009635   \n",
       "\n",
       "            yU_6      yU_7      yU_8      yU_9  ...     yL_12     yL_13  \\\n",
       "0       0.024823  0.023923  0.021796  0.018781  ... -0.007926 -0.004725   \n",
       "1       0.024823  0.023923  0.021796  0.018781  ... -0.007926 -0.004725   \n",
       "2       0.024823  0.023923  0.021796  0.018781  ... -0.007926 -0.004725   \n",
       "3       0.024823  0.023923  0.021796  0.018781  ... -0.007926 -0.004725   \n",
       "4       0.024823  0.023923  0.021796  0.018781  ... -0.007926 -0.004725   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "253189 -0.014932 -0.018353 -0.019393 -0.018142  ... -0.023546 -0.013828   \n",
       "253190 -0.014932 -0.018353 -0.019393 -0.018142  ... -0.023546 -0.013828   \n",
       "253191 -0.014932 -0.018353 -0.019393 -0.018142  ... -0.023546 -0.013828   \n",
       "253192 -0.014932 -0.018353 -0.019393 -0.018142  ... -0.023546 -0.013828   \n",
       "253193 -0.014932 -0.018353 -0.019393 -0.018142  ... -0.023546 -0.013828   \n",
       "\n",
       "           yL_14     yL_15  ReynoldsNumber  MachNumber  alpha     Cl       Cd  \\\n",
       "0      -0.002193 -0.000563          100000         0.1    -10 -0.334  0.16140   \n",
       "1      -0.002193 -0.000563          100000         0.1     -9 -0.392  0.13236   \n",
       "2      -0.002193 -0.000563          100000         0.1     -8 -0.442  0.10163   \n",
       "3      -0.002193 -0.000563          100000         0.1     -7 -0.469  0.07583   \n",
       "4      -0.002193 -0.000563          100000         0.1     -6 -0.459  0.05855   \n",
       "...          ...       ...             ...         ...    ...    ...      ...   \n",
       "253189 -0.006325 -0.001614          500000         0.3      6  0.103  0.06955   \n",
       "253190 -0.006325 -0.001614          500000         0.3      7  0.033  0.05742   \n",
       "253191 -0.006325 -0.001614          500000         0.3      8  0.044  0.06682   \n",
       "253192 -0.006325 -0.001614          500000         0.3      9  0.055  0.08164   \n",
       "253193 -0.006325 -0.001614          500000         0.3     10  0.066  0.07665   \n",
       "\n",
       "           Cm  \n",
       "0       0.001  \n",
       "1       0.001  \n",
       "2       0.001  \n",
       "3       0.001  \n",
       "4       0.001  \n",
       "...       ...  \n",
       "253189  0.031  \n",
       "253190  0.033  \n",
       "253191  0.034  \n",
       "253192  0.035  \n",
       "253193  0.036  \n",
       "\n",
       "[253194 rows x 37 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = assign_airfoil_ids(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb608624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "       598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
       "       611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
       "       624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
       "       637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
       "       650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n",
       "       663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n",
       "       676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n",
       "       689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n",
       "       702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n",
       "       715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n",
       "       728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n",
       "       741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753,\n",
       "       754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766,\n",
       "       767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779,\n",
       "       780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792,\n",
       "       793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805,\n",
       "       806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818,\n",
       "       819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Airfoil_No'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a0638-b1db-4e9f-8bcc-fba784d54511",
   "metadata": {},
   "source": [
    "## RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "66f1cf2f-175d-4619-8906-14d9fbde0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5ffafbb2-9ba9-4c85-9084-f4c34b0f7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique airfoil numbers\n",
    "unique_airfoil_numbers = df['Airfoil_No'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5d439b3b-18d1-4011-8532-1831eda9c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_airfoils, test_airfoils, _, _ = train_test_split(unique_airfoil_numbers, unique_airfoil_numbers, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "95ca5d2c-1287-4920-a539-eb4690198753",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_rows_for_airfoils(train_airfoils)\n",
    "test_df = get_rows_for_airfoils(test_airfoils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2eb23b66-763c-46aa-af0f-943262bd026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Min-Max scaling on the features separately for each fold\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train_df.drop(['Airfoil_No', 'Cl', 'Cd', 'Cm'], axis=1).values)\n",
    "y_train = train_df[['Cl', 'Cd', 'Cm']].values\n",
    "X_test = scaler.transform(test_df.drop(['Airfoil_No', 'Cl', 'Cd', 'Cm'], axis=1).values)\n",
    "y_test = test_df[['Cl', 'Cd', 'Cm']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "445188bc-71c7-41a2-a819-25ba19f870d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lasal Jayawardena\\AppData\\Local\\Temp\\ipykernel_15648\\182243599.py:2: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  regressor = KerasRegressor(build_fn=create_model, epochs=20, batch_size=128, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# Create a KerasRegressor wrapper for use with RandomizedSearchCV\n",
    "regressor = KerasRegressor(build_fn=create_model, epochs=20, batch_size=128, verbose=0)\n",
    "\n",
    "# Define the hyperparameter distributions for tuning\n",
    "param_dist = {\n",
    "    'neurons1': [512, 256, 128],  # Neurons with order-of-2 range\n",
    "    'neurons2': [256, 128, 64],  # Neurons with order-of-2 range\n",
    "    'neurons3': [64, 32, 16],  # Neurons with order-of-2 range\n",
    "    'neurons4': [32, 16, 8],  # Neurons with order-of-2 range\n",
    "}\n",
    "\n",
    "# Create the RandomizedSearchCV object with a maximum of 20 parameter combinations\n",
    "random_search = RandomizedSearchCV(estimator=regressor, param_distributions=param_dist, scoring='neg_mean_squared_error', n_iter=20, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5cf50a93-3bc1-4d6e-afb3-56b150b11f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hyperparameter tuning with early stopping and evaluation on the test set\n",
    "random_result = random_search.fit(X_train, y_train, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0b177b50-d791-4893-beaf-4c788496e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812/812 [==============================] - 1s 1ms/step\n",
      "Test RMSE: 0.018075607436702727\n"
     ]
    }
   ],
   "source": [
    "# Get the best model for the current fold\n",
    "best_model = random_result.best_estimator_.model\n",
    "best_scaler = scaler\n",
    "\n",
    "# Save the best model and scaler for the current fold\n",
    "best_model.save(f'./search_assets/best_model_fold_{fold}.h5')\n",
    "joblib.dump(best_scaler, f'./search_assets/best_min_max_scaler_fold_{fold}.pkl')\n",
    "\n",
    "# Evaluate the best model on the test data for the current fold\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6816a488-eb2f-4dce-b966-daad9b9afef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neurons4': 8, 'neurons3': 64, 'neurons2': 256, 'neurons1': 512}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random_result.cv_results_\n",
    "random_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "38470bfd-77e5-4c82-9c33-fe80db0f5859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([75.97858825, 71.14025769, 64.90447259, 65.81463943, 68.04780111,\n",
       "        72.96433668, 69.68225107, 68.33149505, 62.20927987, 63.47398815,\n",
       "        66.76915426, 67.04433661, 64.97910404, 69.35469065, 71.39298973,\n",
       "        66.07277493, 63.45933309, 66.68185115, 66.01372724, 64.54137521]),\n",
       " 'std_fit_time': array([3.43950954, 3.43002975, 0.90212553, 1.22272113, 2.47892632,\n",
       "        0.62948257, 1.0289225 , 1.70756711, 0.95669067, 0.84773743,\n",
       "        2.1981148 , 0.68241706, 1.05659811, 4.7597037 , 2.71317689,\n",
       "        1.74066708, 0.25026036, 2.48444038, 1.26591066, 0.7706904 ]),\n",
       " 'mean_score_time': array([0.6275744 , 0.55690775, 0.55606308, 0.5622128 , 0.63836412,\n",
       "        0.66432729, 0.58277574, 0.58794823, 0.53557243, 0.54319835,\n",
       "        0.56676154, 0.54236388, 0.54494276, 0.58273721, 0.58283663,\n",
       "        0.53511467, 0.54410267, 0.5302856 , 0.53489561, 0.52087221]),\n",
       " 'std_score_time': array([0.05393743, 0.02199572, 0.03624535, 0.03648619, 0.06004387,\n",
       "        0.05263532, 0.02650326, 0.06732779, 0.02048069, 0.01574675,\n",
       "        0.03023581, 0.02306006, 0.02237412, 0.05887002, 0.03102917,\n",
       "        0.01643101, 0.01587394, 0.02280707, 0.02056214, 0.00693338]),\n",
       " 'param_neurons4': masked_array(data=[16, 8, 8, 16, 32, 16, 16, 32, 32, 8, 8, 16, 8, 8, 32,\n",
       "                    8, 8, 16, 32, 32],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_neurons3': masked_array(data=[64, 32, 32, 16, 64, 64, 16, 32, 32, 64, 64, 64, 16, 32,\n",
       "                    16, 16, 64, 64, 16, 16],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_neurons2': masked_array(data=[128, 64, 64, 128, 64, 256, 128, 64, 256, 128, 64, 256,\n",
       "                    64, 128, 256, 256, 256, 256, 128, 128],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_neurons1': masked_array(data=[256, 512, 256, 256, 256, 128, 512, 512, 512, 128, 512,\n",
       "                    256, 256, 128, 128, 128, 512, 512, 128, 512],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'neurons4': 16, 'neurons3': 64, 'neurons2': 128, 'neurons1': 256},\n",
       "  {'neurons4': 8, 'neurons3': 32, 'neurons2': 64, 'neurons1': 512},\n",
       "  {'neurons4': 8, 'neurons3': 32, 'neurons2': 64, 'neurons1': 256},\n",
       "  {'neurons4': 16, 'neurons3': 16, 'neurons2': 128, 'neurons1': 256},\n",
       "  {'neurons4': 32, 'neurons3': 64, 'neurons2': 64, 'neurons1': 256},\n",
       "  {'neurons4': 16, 'neurons3': 64, 'neurons2': 256, 'neurons1': 128},\n",
       "  {'neurons4': 16, 'neurons3': 16, 'neurons2': 128, 'neurons1': 512},\n",
       "  {'neurons4': 32, 'neurons3': 32, 'neurons2': 64, 'neurons1': 512},\n",
       "  {'neurons4': 32, 'neurons3': 32, 'neurons2': 256, 'neurons1': 512},\n",
       "  {'neurons4': 8, 'neurons3': 64, 'neurons2': 128, 'neurons1': 128},\n",
       "  {'neurons4': 8, 'neurons3': 64, 'neurons2': 64, 'neurons1': 512},\n",
       "  {'neurons4': 16, 'neurons3': 64, 'neurons2': 256, 'neurons1': 256},\n",
       "  {'neurons4': 8, 'neurons3': 16, 'neurons2': 64, 'neurons1': 256},\n",
       "  {'neurons4': 8, 'neurons3': 32, 'neurons2': 128, 'neurons1': 128},\n",
       "  {'neurons4': 32, 'neurons3': 16, 'neurons2': 256, 'neurons1': 128},\n",
       "  {'neurons4': 8, 'neurons3': 16, 'neurons2': 256, 'neurons1': 128},\n",
       "  {'neurons4': 8, 'neurons3': 64, 'neurons2': 256, 'neurons1': 512},\n",
       "  {'neurons4': 16, 'neurons3': 64, 'neurons2': 256, 'neurons1': 512},\n",
       "  {'neurons4': 32, 'neurons3': 16, 'neurons2': 128, 'neurons1': 128},\n",
       "  {'neurons4': 32, 'neurons3': 16, 'neurons2': 128, 'neurons1': 512}],\n",
       " 'split0_test_score': array([-0.00028391, -0.00036634, -0.0008203 , -0.00031874, -0.00027157,\n",
       "        -0.00026234, -0.00036318, -0.00033075, -0.00025739, -0.00026157,\n",
       "        -0.0002966 , -0.00034937, -0.00044037, -0.00035308, -0.00035148,\n",
       "        -0.00030823, -0.00025756, -0.0003618 , -0.00064156, -0.00029743]),\n",
       " 'split1_test_score': array([-0.00048308, -0.00044476, -0.00033985, -0.00067465, -0.00038498,\n",
       "        -0.00055638, -0.0008974 , -0.00035183, -0.00044068, -0.00042409,\n",
       "        -0.00061213, -0.00042729, -0.0003745 , -0.00091862, -0.00032753,\n",
       "        -0.00051553, -0.0003054 , -0.00056854, -0.0003742 , -0.00078173]),\n",
       " 'split2_test_score': array([-0.00174995, -0.00090773, -0.0007392 , -0.00076434, -0.00109413,\n",
       "        -0.00077701, -0.00065373, -0.0011797 , -0.0010043 , -0.00085475,\n",
       "        -0.00074102, -0.0009235 , -0.00108167, -0.00128685, -0.00063747,\n",
       "        -0.0007366 , -0.00073433, -0.00053281, -0.00072266, -0.00095286]),\n",
       " 'split3_test_score': array([-0.00134756, -0.00093765, -0.00142619, -0.00091929, -0.00114346,\n",
       "        -0.00139717, -0.00096994, -0.00098666, -0.0008541 , -0.00104142,\n",
       "        -0.0010511 , -0.00091576, -0.00164205, -0.00081661, -0.00099489,\n",
       "        -0.00120974, -0.00087586, -0.00086545, -0.0021404 , -0.00091607]),\n",
       " 'split4_test_score': array([-0.00032286, -0.00054041, -0.00035772, -0.00040652, -0.00063719,\n",
       "        -0.00037209, -0.00034861, -0.00033538, -0.00051819, -0.00032441,\n",
       "        -0.00069915, -0.00033471, -0.0010171 , -0.0004298 , -0.00043353,\n",
       "        -0.00034022, -0.00033246, -0.00075746, -0.00083166, -0.00063432]),\n",
       " 'mean_test_score': array([-0.00083747, -0.00063938, -0.00073665, -0.00061671, -0.00070627,\n",
       "        -0.000673  , -0.00064657, -0.00063686, -0.00061493, -0.00058125,\n",
       "        -0.00068   , -0.00059013, -0.00091114, -0.00076099, -0.00054898,\n",
       "        -0.00062206, -0.00050112, -0.00061721, -0.0009421 , -0.00071648]),\n",
       " 'std_test_score': array([0.00059828, 0.00023799, 0.00039586, 0.00022346, 0.00035736,\n",
       "        0.000402  , 0.00025947, 0.00036956, 0.00027436, 0.00030965,\n",
       "        0.00024224, 0.00027088, 0.00046565, 0.00034075, 0.00024821,\n",
       "        0.00033099, 0.00025334, 0.00017665, 0.00061792, 0.00023762]),\n",
       " 'rank_test_score': array([18, 10, 16,  6, 14, 12, 11,  9,  5,  3, 13,  4, 19, 17,  2,  8,  1,\n",
       "         7, 20, 15])}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_result.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3009dc-83ba-4e89-ac1d-1a5a17310efd",
   "metadata": {},
   "source": [
    "## KFOLD and RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b0bf0297-73bf-4cd5-9784-8ff0ee2f0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f5afb271-2617-4a69-a395-8bd7e67898d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c4d3b611-a3ca-4d81-87a3-77ac4cf846b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store model, scaler, and history objects for each fold\n",
    "models = []\n",
    "scalers = []\n",
    "histories = []\n",
    "\n",
    "# Define a function to get rows of the DataFrame for a given array of airfoil numbers\n",
    "def get_rows_for_airfoils(airfoil_numbers):\n",
    "    return df[df['Airfoil_No'].isin(airfoil_numbers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f1b390ba-6c7d-4d58-85df-12f7237e3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the Keras model with hyperparameters\n",
    "def create_model(neurons1=512, neurons2=256, neurons3=64, neurons4=8):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, activation='relu'))\n",
    "    model.add(Dense(neurons2, activation='relu'))\n",
    "    model.add(Dense(neurons3, activation='relu'))\n",
    "    model.add(Dense(neurons4, activation='relu'))\n",
    "    model.add(Dense(3))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a4c86efc-afc4-44b2-a354-76810c4f195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "7a4e2e31-3d4f-402d-8a08-12c472251b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 / 10\n",
      "812/812 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:41, 101.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Test RMSE: 0.020048656153202282\n",
      "Fold 2 / 10\n",
      "797/797 [==============================] - 1s 978us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [04:36, 144.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Test RMSE: 0.012021563744327479\n",
      "Fold 3 / 10\n",
      "792/792 [==============================] - 1s 997us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [06:42, 136.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Test RMSE: 0.013969964342428252\n",
      "Fold 4 / 10\n",
      "788/788 [==============================] - 1s 999us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [11:41, 200.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Test RMSE: 0.012778653765117318\n",
      "Fold 5 / 10\n",
      "786/786 [==============================] - 3s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [15:24, 208.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Test RMSE: 0.014046142547115436\n",
      "Fold 6 / 10\n",
      "786/786 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [18:25, 199.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6 Test RMSE: 0.013198569777072405\n",
      "Fold 7 / 10\n",
      "785/785 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [21:02, 185.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7 Test RMSE: 0.014518572390069132\n",
      "Fold 8 / 10\n",
      "791/791 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [21:59, 144.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8 Test RMSE: 0.02133214459179447\n",
      "Fold 9 / 10\n",
      "792/792 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [24:46, 151.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9 Test RMSE: 0.014117105733354165\n",
      "Fold 10 / 10\n",
      "788/788 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [27:35, 165.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 10 Test RMSE: 0.012839338492140997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_indices, test_indices) in tqdm(enumerate(stratified_kfold.split(unique_airfoil_numbers, unique_airfoil_numbers))):\n",
    "    print(f\"Fold {fold + 1} / {n_splits}\")\n",
    "\n",
    "    # Get the airfoil numbers for training and testing in the current fold\n",
    "    train_airfoils = unique_airfoil_numbers[train_indices]\n",
    "    test_airfoils = unique_airfoil_numbers[test_indices]\n",
    "\n",
    "    # Get the rows of the DataFrame for the selected airfoil numbers\n",
    "    train_df = get_rows_for_airfoils(train_airfoils)\n",
    "    test_df = get_rows_for_airfoils(test_airfoils)\n",
    "\n",
    "    # Perform Min-Max scaling on the features separately for each fold\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(train_df.drop(['Airfoil_No', 'Cl', 'Cd', 'Cm'], axis=1).values)\n",
    "    y_train = train_df[['Cl', 'Cd', 'Cm']].values\n",
    "    X_test = scaler.transform(test_df.drop(['Airfoil_No', 'Cl', 'Cd', 'Cm'], axis=1).values)\n",
    "    y_test = test_df[['Cl', 'Cd', 'Cm']].values\n",
    "\n",
    "    # Perform hyperparameter tuning with early stopping and evaluation on the test set\n",
    "    model = create_model()\n",
    "    history = model.fit(X_train, y_train, batch_size=128, validation_data=(X_test, y_test), epochs=100, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Get the best model for the current fold\n",
    "    best_model = model\n",
    "    best_scaler = scaler\n",
    "\n",
    "    # Save the best model and scaler for the current fold\n",
    "    best_model.save(f'./training_assets/best_model_fold_{fold}.h5')\n",
    "    joblib.dump(best_scaler, f'./training_assets/best_min_max_scaler_fold_{fold}.pkl')\n",
    "\n",
    "    # Evaluate the best model on the test data for the current fold\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"Fold {fold + 1} Test RMSE: {rmse}\")\n",
    "\n",
    "    # Save the history object for the current fold\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "836c4d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x273b3c83010>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histories[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17897b85-bda3-438f-b550-497d00fa2433",
   "metadata": {},
   "source": [
    "# Train Best Model with Best Split with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b1051503-5b13-4ea5-a90c-71eb1185bf33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Airfoil_No', 'yU_1', 'yU_2', 'yU_3', 'yU_4', 'yU_5', 'yU_6', 'yU_7',\n",
       "       'yU_8', 'yU_9', 'yU_10', 'yU_11', 'yU_12', 'yU_13', 'yU_14', 'yU_15',\n",
       "       'yL_1', 'yL_2', 'yL_3', 'yL_4', 'yL_5', 'yL_6', 'yL_7', 'yL_8', 'yL_9',\n",
       "       'yL_10', 'yL_11', 'yL_12', 'yL_13', 'yL_14', 'yL_15', 'ReynoldsNumber',\n",
       "       'MachNumber', 'alpha', 'Cl', 'Cd', 'Cm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f7b2bc4d-f8ee-45e2-9c9f-31f1674b1818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the Keras model with hyperparameters\n",
    "def create_model(neurons1=512, neurons2=256, neurons3=64, neurons4=8):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, activation='relu'))\n",
    "    model.add(Dense(neurons2, activation='relu'))\n",
    "    model.add(Dense(neurons3, activation='relu'))\n",
    "    model.add(Dense(neurons4, activation='relu'))\n",
    "    model.add(Dense(3))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "03294323-dc01-4926-8404-287418c90677",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "07355cbd-80b0-4903-80cc-b4cefca8d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = list(stratified_kfold.split(unique_airfoil_numbers, unique_airfoil_numbers)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5cec8543-d47a-420e-b9b5-d1299d310a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_split = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d2967cbb-071e-41a5-bccc-1b74e094f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices = best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a3963085-e8eb-43f4-93fe-b9a7f96c2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10,)\n",
    "# Create a ModelCheckpoint callback to save the best model based on validation loss\n",
    "checkpoint = ModelCheckpoint('./best_model/best_model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "deffaad7-4115-413f-8743-1e303bc1c590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1773/1779 [============================>.] - ETA: 0s - loss: 0.0094\n",
      "Epoch 1: val_loss improved from inf to 0.00097, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 0.0094 - val_loss: 9.7499e-04\n",
      "Epoch 2/100\n",
      "1759/1779 [============================>.] - ETA: 0s - loss: 0.0011\n",
      "Epoch 2: val_loss improved from 0.00097 to 0.00067, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 0.0011 - val_loss: 6.7173e-04\n",
      "Epoch 3/100\n",
      "1766/1779 [============================>.] - ETA: 0s - loss: 9.6066e-04\n",
      "Epoch 3: val_loss improved from 0.00067 to 0.00061, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 9.6099e-04 - val_loss: 6.1246e-04\n",
      "Epoch 4/100\n",
      "1770/1779 [============================>.] - ETA: 0s - loss: 8.3608e-04\n",
      "Epoch 4: val_loss improved from 0.00061 to 0.00060, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 8.3593e-04 - val_loss: 5.9738e-04\n",
      "Epoch 5/100\n",
      "1765/1779 [============================>.] - ETA: 0s - loss: 7.7764e-04\n",
      "Epoch 5: val_loss did not improve from 0.00060\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 7.7662e-04 - val_loss: 6.0719e-04\n",
      "Epoch 6/100\n",
      "1769/1779 [============================>.] - ETA: 0s - loss: 6.9345e-04\n",
      "Epoch 6: val_loss improved from 0.00060 to 0.00046, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 6.9331e-04 - val_loss: 4.6479e-04\n",
      "Epoch 7/100\n",
      "1779/1779 [==============================] - ETA: 0s - loss: 6.2344e-04\n",
      "Epoch 7: val_loss improved from 0.00046 to 0.00042, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 6.2344e-04 - val_loss: 4.2459e-04\n",
      "Epoch 8/100\n",
      "1774/1779 [============================>.] - ETA: 0s - loss: 5.0541e-04\n",
      "Epoch 8: val_loss did not improve from 0.00042\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 5.0570e-04 - val_loss: 8.6302e-04\n",
      "Epoch 9/100\n",
      "1760/1779 [============================>.] - ETA: 0s - loss: 4.1035e-04\n",
      "Epoch 9: val_loss improved from 0.00042 to 0.00023, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 4.0930e-04 - val_loss: 2.2612e-04\n",
      "Epoch 10/100\n",
      "1779/1779 [==============================] - ETA: 0s - loss: 4.0576e-04\n",
      "Epoch 10: val_loss did not improve from 0.00023\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 4.0576e-04 - val_loss: 2.8339e-04\n",
      "Epoch 11/100\n",
      "1761/1779 [============================>.] - ETA: 0s - loss: 3.8001e-04\n",
      "Epoch 11: val_loss did not improve from 0.00023\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 3.7946e-04 - val_loss: 5.0244e-04\n",
      "Epoch 12/100\n",
      "1776/1779 [============================>.] - ETA: 0s - loss: 3.7260e-04\n",
      "Epoch 12: val_loss did not improve from 0.00023\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 3.7257e-04 - val_loss: 3.2219e-04\n",
      "Epoch 13/100\n",
      "1767/1779 [============================>.] - ETA: 0s - loss: 3.3898e-04\n",
      "Epoch 13: val_loss did not improve from 0.00023\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 3.3835e-04 - val_loss: 2.8352e-04\n",
      "Epoch 14/100\n",
      "1770/1779 [============================>.] - ETA: 0s - loss: 3.4164e-04\n",
      "Epoch 14: val_loss did not improve from 0.00023\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 3.4215e-04 - val_loss: 4.1957e-04\n",
      "Epoch 15/100\n",
      "1765/1779 [============================>.] - ETA: 0s - loss: 3.1934e-04\n",
      "Epoch 15: val_loss did not improve from 0.00023\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 3.2086e-04 - val_loss: 2.5591e-04\n",
      "Epoch 16/100\n",
      "1767/1779 [============================>.] - ETA: 0s - loss: 3.0456e-04\n",
      "Epoch 16: val_loss did not improve from 0.00023\n",
      "1779/1779 [==============================] - 4s 3ms/step - loss: 3.0504e-04 - val_loss: 3.0374e-04\n",
      "Epoch 17/100\n",
      "1769/1779 [============================>.] - ETA: 0s - loss: 3.0394e-04\n",
      "Epoch 17: val_loss did not improve from 0.00023\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 3.0365e-04 - val_loss: 2.6499e-04\n",
      "Epoch 18/100\n",
      "1768/1779 [============================>.] - ETA: 0s - loss: 2.8643e-04\n",
      "Epoch 18: val_loss improved from 0.00023 to 0.00021, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.8675e-04 - val_loss: 2.0894e-04\n",
      "Epoch 19/100\n",
      "1761/1779 [============================>.] - ETA: 0s - loss: 2.8077e-04\n",
      "Epoch 19: val_loss did not improve from 0.00021\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.8079e-04 - val_loss: 3.5037e-04\n",
      "Epoch 20/100\n",
      "1766/1779 [============================>.] - ETA: 0s - loss: 2.7505e-04\n",
      "Epoch 20: val_loss did not improve from 0.00021\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.7549e-04 - val_loss: 2.1652e-04\n",
      "Epoch 21/100\n",
      "1774/1779 [============================>.] - ETA: 0s - loss: 2.6667e-04\n",
      "Epoch 21: val_loss did not improve from 0.00021\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.6683e-04 - val_loss: 3.2769e-04\n",
      "Epoch 22/100\n",
      "1759/1779 [============================>.] - ETA: 0s - loss: 2.6500e-04\n",
      "Epoch 22: val_loss did not improve from 0.00021\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.6551e-04 - val_loss: 2.6584e-04\n",
      "Epoch 23/100\n",
      "1771/1779 [============================>.] - ETA: 0s - loss: 2.6332e-04\n",
      "Epoch 23: val_loss did not improve from 0.00021\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.6338e-04 - val_loss: 2.2958e-04\n",
      "Epoch 24/100\n",
      "1772/1779 [============================>.] - ETA: 0s - loss: 2.4330e-04\n",
      "Epoch 24: val_loss did not improve from 0.00021\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.4337e-04 - val_loss: 2.9878e-04\n",
      "Epoch 25/100\n",
      "1775/1779 [============================>.] - ETA: 0s - loss: 2.6086e-04\n",
      "Epoch 25: val_loss did not improve from 0.00021\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.6089e-04 - val_loss: 2.1015e-04\n",
      "Epoch 26/100\n",
      "1770/1779 [============================>.] - ETA: 0s - loss: 2.4455e-04\n",
      "Epoch 26: val_loss did not improve from 0.00021\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.4449e-04 - val_loss: 2.1910e-04\n",
      "Epoch 27/100\n",
      "1764/1779 [============================>.] - ETA: 0s - loss: 2.4155e-04\n",
      "Epoch 27: val_loss did not improve from 0.00021\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.4129e-04 - val_loss: 2.3098e-04\n",
      "Epoch 28/100\n",
      "1767/1779 [============================>.] - ETA: 0s - loss: 2.3726e-04\n",
      "Epoch 28: val_loss improved from 0.00021 to 0.00021, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.3787e-04 - val_loss: 2.0670e-04\n",
      "Epoch 29/100\n",
      "1759/1779 [============================>.] - ETA: 0s - loss: 2.3653e-04\n",
      "Epoch 29: val_loss did not improve from 0.00021\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.3737e-04 - val_loss: 2.5826e-04\n",
      "Epoch 30/100\n",
      "1762/1779 [============================>.] - ETA: 0s - loss: 2.2492e-04\n",
      "Epoch 30: val_loss did not improve from 0.00021\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.2534e-04 - val_loss: 3.1560e-04\n",
      "Epoch 31/100\n",
      "1770/1779 [============================>.] - ETA: 0s - loss: 2.2832e-04\n",
      "Epoch 31: val_loss improved from 0.00021 to 0.00017, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.2791e-04 - val_loss: 1.6832e-04\n",
      "Epoch 32/100\n",
      "1772/1779 [============================>.] - ETA: 0s - loss: 2.2563e-04\n",
      "Epoch 32: val_loss did not improve from 0.00017\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.2585e-04 - val_loss: 2.2736e-04\n",
      "Epoch 33/100\n",
      "1779/1779 [==============================] - ETA: 0s - loss: 2.2465e-04\n",
      "Epoch 33: val_loss improved from 0.00017 to 0.00017, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.2465e-04 - val_loss: 1.6635e-04\n",
      "Epoch 34/100\n",
      "1770/1779 [============================>.] - ETA: 0s - loss: 2.1292e-04\n",
      "Epoch 34: val_loss did not improve from 0.00017\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.1294e-04 - val_loss: 2.0773e-04\n",
      "Epoch 35/100\n",
      "1769/1779 [============================>.] - ETA: 0s - loss: 2.1233e-04\n",
      "Epoch 35: val_loss did not improve from 0.00017\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.1199e-04 - val_loss: 1.6817e-04\n",
      "Epoch 36/100\n",
      "1773/1779 [============================>.] - ETA: 0s - loss: 2.0685e-04\n",
      "Epoch 36: val_loss did not improve from 0.00017\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.0682e-04 - val_loss: 1.7725e-04\n",
      "Epoch 37/100\n",
      "1760/1779 [============================>.] - ETA: 0s - loss: 2.0877e-04\n",
      "Epoch 37: val_loss did not improve from 0.00017\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.0856e-04 - val_loss: 1.7768e-04\n",
      "Epoch 38/100\n",
      "1770/1779 [============================>.] - ETA: 0s - loss: 2.1111e-04\n",
      "Epoch 38: val_loss did not improve from 0.00017\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.1086e-04 - val_loss: 1.8313e-04\n",
      "Epoch 39/100\n",
      "1762/1779 [============================>.] - ETA: 0s - loss: 2.0953e-04\n",
      "Epoch 39: val_loss improved from 0.00017 to 0.00015, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.0903e-04 - val_loss: 1.4951e-04\n",
      "Epoch 40/100\n",
      "1769/1779 [============================>.] - ETA: 0s - loss: 1.9434e-04\n",
      "Epoch 40: val_loss improved from 0.00015 to 0.00014, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.9443e-04 - val_loss: 1.4321e-04\n",
      "Epoch 41/100\n",
      "1758/1779 [============================>.] - ETA: 0s - loss: 2.0677e-04\n",
      "Epoch 41: val_loss did not improve from 0.00014\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.0597e-04 - val_loss: 1.5144e-04\n",
      "Epoch 42/100\n",
      "1765/1779 [============================>.] - ETA: 0s - loss: 2.0269e-04\n",
      "Epoch 42: val_loss did not improve from 0.00014\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 2.0263e-04 - val_loss: 1.4636e-04\n",
      "Epoch 43/100\n",
      "1778/1779 [============================>.] - ETA: 0s - loss: 1.9168e-04\n",
      "Epoch 43: val_loss did not improve from 0.00014\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.9169e-04 - val_loss: 1.7583e-04\n",
      "Epoch 44/100\n",
      "1776/1779 [============================>.] - ETA: 0s - loss: 1.9957e-04\n",
      "Epoch 44: val_loss did not improve from 0.00014\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.9957e-04 - val_loss: 1.6961e-04\n",
      "Epoch 45/100\n",
      "1761/1779 [============================>.] - ETA: 0s - loss: 1.8881e-04\n",
      "Epoch 45: val_loss did not improve from 0.00014\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.8901e-04 - val_loss: 1.4437e-04\n",
      "Epoch 46/100\n",
      "1775/1779 [============================>.] - ETA: 0s - loss: 1.9727e-04\n",
      "Epoch 46: val_loss did not improve from 0.00014\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.9776e-04 - val_loss: 2.7609e-04\n",
      "Epoch 47/100\n",
      "1767/1779 [============================>.] - ETA: 0s - loss: 1.8587e-04\n",
      "Epoch 47: val_loss improved from 0.00014 to 0.00014, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.8571e-04 - val_loss: 1.4035e-04\n",
      "Epoch 48/100\n",
      "1762/1779 [============================>.] - ETA: 0s - loss: 1.8680e-04\n",
      "Epoch 48: val_loss did not improve from 0.00014\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.8736e-04 - val_loss: 2.9755e-04\n",
      "Epoch 49/100\n",
      "1776/1779 [============================>.] - ETA: 0s - loss: 1.9170e-04\n",
      "Epoch 49: val_loss did not improve from 0.00014\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.9163e-04 - val_loss: 1.8653e-04\n",
      "Epoch 50/100\n",
      "1759/1779 [============================>.] - ETA: 0s - loss: 1.7942e-04\n",
      "Epoch 50: val_loss did not improve from 0.00014\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.7934e-04 - val_loss: 1.9107e-04\n",
      "Epoch 51/100\n",
      "1776/1779 [============================>.] - ETA: 0s - loss: 1.8402e-04\n",
      "Epoch 51: val_loss did not improve from 0.00014\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.8393e-04 - val_loss: 1.7401e-04\n",
      "Epoch 52/100\n",
      "1761/1779 [============================>.] - ETA: 0s - loss: 1.8404e-04\n",
      "Epoch 52: val_loss did not improve from 0.00014\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.8470e-04 - val_loss: 2.2893e-04\n",
      "Epoch 53/100\n",
      "1770/1779 [============================>.] - ETA: 0s - loss: 1.8590e-04\n",
      "Epoch 53: val_loss improved from 0.00014 to 0.00013, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.8586e-04 - val_loss: 1.3250e-04\n",
      "Epoch 54/100\n",
      "1763/1779 [============================>.] - ETA: 0s - loss: 1.8374e-04\n",
      "Epoch 54: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.8328e-04 - val_loss: 1.9037e-04\n",
      "Epoch 55/100\n",
      "1778/1779 [============================>.] - ETA: 0s - loss: 1.6915e-04\n",
      "Epoch 55: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.6916e-04 - val_loss: 1.8455e-04\n",
      "Epoch 56/100\n",
      "1772/1779 [============================>.] - ETA: 0s - loss: 1.7746e-04\n",
      "Epoch 56: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.7764e-04 - val_loss: 1.6727e-04\n",
      "Epoch 57/100\n",
      "1775/1779 [============================>.] - ETA: 0s - loss: 1.7759e-04\n",
      "Epoch 57: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.7759e-04 - val_loss: 2.5253e-04\n",
      "Epoch 58/100\n",
      "1766/1779 [============================>.] - ETA: 0s - loss: 1.7795e-04\n",
      "Epoch 58: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.7812e-04 - val_loss: 1.5379e-04\n",
      "Epoch 59/100\n",
      "1760/1779 [============================>.] - ETA: 0s - loss: 1.6650e-04\n",
      "Epoch 59: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.6594e-04 - val_loss: 1.5462e-04\n",
      "Epoch 60/100\n",
      "1774/1779 [============================>.] - ETA: 0s - loss: 1.6387e-04\n",
      "Epoch 60: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 6s 3ms/step - loss: 1.6384e-04 - val_loss: 1.4267e-04\n",
      "Epoch 61/100\n",
      "1775/1779 [============================>.] - ETA: 0s - loss: 1.7619e-04\n",
      "Epoch 61: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.7608e-04 - val_loss: 1.3754e-04\n",
      "Epoch 62/100\n",
      "1770/1779 [============================>.] - ETA: 0s - loss: 1.6294e-04\n",
      "Epoch 62: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.6298e-04 - val_loss: 1.6387e-04\n",
      "Epoch 63/100\n",
      "1776/1779 [============================>.] - ETA: 0s - loss: 1.6855e-04\n",
      "Epoch 63: val_loss improved from 0.00013 to 0.00013, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.6846e-04 - val_loss: 1.2827e-04\n",
      "Epoch 64/100\n",
      "1760/1779 [============================>.] - ETA: 0s - loss: 1.6575e-04\n",
      "Epoch 64: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.6569e-04 - val_loss: 1.3507e-04\n",
      "Epoch 65/100\n",
      "1762/1779 [============================>.] - ETA: 0s - loss: 1.6763e-04\n",
      "Epoch 65: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.6716e-04 - val_loss: 1.3760e-04\n",
      "Epoch 66/100\n",
      "1767/1779 [============================>.] - ETA: 0s - loss: 1.6400e-04\n",
      "Epoch 66: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.6408e-04 - val_loss: 1.5321e-04\n",
      "Epoch 67/100\n",
      "1770/1779 [============================>.] - ETA: 0s - loss: 1.6185e-04\n",
      "Epoch 67: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.6183e-04 - val_loss: 1.4664e-04\n",
      "Epoch 68/100\n",
      "1761/1779 [============================>.] - ETA: 0s - loss: 1.6589e-04\n",
      "Epoch 68: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.6642e-04 - val_loss: 2.1570e-04\n",
      "Epoch 69/100\n",
      "1775/1779 [============================>.] - ETA: 0s - loss: 1.5483e-04\n",
      "Epoch 69: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.5501e-04 - val_loss: 1.6725e-04\n",
      "Epoch 70/100\n",
      "1761/1779 [============================>.] - ETA: 0s - loss: 1.5719e-04\n",
      "Epoch 70: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.5695e-04 - val_loss: 1.4775e-04\n",
      "Epoch 71/100\n",
      "1761/1779 [============================>.] - ETA: 0s - loss: 1.5693e-04\n",
      "Epoch 71: val_loss did not improve from 0.00013\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.5640e-04 - val_loss: 1.6515e-04\n",
      "Epoch 72/100\n",
      "1777/1779 [============================>.] - ETA: 0s - loss: 1.5633e-04\n",
      "Epoch 72: val_loss improved from 0.00013 to 0.00012, saving model to ./best_model\\best_model.h5\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.5628e-04 - val_loss: 1.1739e-04\n",
      "Epoch 73/100\n",
      "1767/1779 [============================>.] - ETA: 0s - loss: 1.5684e-04\n",
      "Epoch 73: val_loss did not improve from 0.00012\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.5779e-04 - val_loss: 1.7523e-04\n",
      "Epoch 74/100\n",
      "1767/1779 [============================>.] - ETA: 0s - loss: 1.6050e-04\n",
      "Epoch 74: val_loss did not improve from 0.00012\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.6055e-04 - val_loss: 1.5825e-04\n",
      "Epoch 75/100\n",
      "1757/1779 [============================>.] - ETA: 0s - loss: 1.6024e-04\n",
      "Epoch 75: val_loss did not improve from 0.00012\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.6027e-04 - val_loss: 1.4777e-04\n",
      "Epoch 76/100\n",
      "1765/1779 [============================>.] - ETA: 0s - loss: 1.6452e-04\n",
      "Epoch 76: val_loss did not improve from 0.00012\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.6439e-04 - val_loss: 1.2357e-04\n",
      "Epoch 77/100\n",
      "1763/1779 [============================>.] - ETA: 0s - loss: 1.5005e-04\n",
      "Epoch 77: val_loss did not improve from 0.00012\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.4979e-04 - val_loss: 1.2624e-04\n",
      "Epoch 78/100\n",
      "1774/1779 [============================>.] - ETA: 0s - loss: 1.5289e-04\n",
      "Epoch 78: val_loss did not improve from 0.00012\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.5278e-04 - val_loss: 2.4995e-04\n",
      "Epoch 79/100\n",
      "1768/1779 [============================>.] - ETA: 0s - loss: 1.4850e-04\n",
      "Epoch 79: val_loss did not improve from 0.00012\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.4813e-04 - val_loss: 1.1884e-04\n",
      "Epoch 80/100\n",
      "1774/1779 [============================>.] - ETA: 0s - loss: 1.5848e-04\n",
      "Epoch 80: val_loss did not improve from 0.00012\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.5870e-04 - val_loss: 1.8475e-04\n",
      "Epoch 81/100\n",
      "1761/1779 [============================>.] - ETA: 0s - loss: 1.5368e-04\n",
      "Epoch 81: val_loss did not improve from 0.00012\n",
      "1779/1779 [==============================] - 5s 3ms/step - loss: 1.5341e-04 - val_loss: 1.7727e-04\n",
      "Epoch 82/100\n",
      "1769/1779 [============================>.] - ETA: 0s - loss: 1.4737e-04\n",
      "Epoch 82: val_loss did not improve from 0.00012\n",
      "1779/1779 [==============================] - 6s 3ms/step - loss: 1.4730e-04 - val_loss: 1.5994e-04\n"
     ]
    }
   ],
   "source": [
    "# Get the airfoil numbers for training and testing in the current fold\n",
    "train_airfoils = unique_airfoil_numbers[train_indices]\n",
    "test_airfoils = unique_airfoil_numbers[test_indices]\n",
    "\n",
    "# Get the rows of the DataFrame for the selected airfoil numbers\n",
    "train_df = get_rows_for_airfoils(train_airfoils)\n",
    "test_df = get_rows_for_airfoils(test_airfoils)\n",
    "\n",
    "# Perform Min-Max scaling on the features separately for each fold\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train_df.drop(['Airfoil_No', 'Cl', 'Cd', 'Cm'], axis=1).values)\n",
    "y_train = train_df[['Cl', 'Cd', 'Cm']].values\n",
    "X_test = scaler.transform(test_df.drop(['Airfoil_No', 'Cl', 'Cd', 'Cm'], axis=1).values)\n",
    "y_test = test_df[['Cl', 'Cd', 'Cm']].values\n",
    "\n",
    "# Perform hyperparameter tuning with early stopping and evaluation on the test set\n",
    "model = create_model()\n",
    "history = model.fit(X_train, y_train, batch_size=128, validation_data=(X_test, y_test), epochs=100, callbacks=[early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cd8b1d7e-1d36-4904-8ce3-b6e3d9280a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss = pd.DataFrame(history.history)\n",
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ee77f0-52ba-49e2-b7d6-346c6bcd1dbe",
   "metadata": {},
   "source": [
    "## Save Assets for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e0763e4d-96fa-4f2e-88bf-71ecf7d61684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5cd5eb48-c3f7-4f38-a653-1b9c2c29a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(\"./best_model/best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e01f94c8-57b1-4516-b711-8b818e5a5701",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(\"./best_model/best_model15.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c57c98db-439f-4759-9ae5-8e4a7c5695da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./best_model/best_scaler15.pkl']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler, f'./best_model/best_scaler15.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ead52644-4055-4873-a06b-521434f4e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e681db41-5fd3-4e99-946e-4b8d534c226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./best_model/best_scaler15.obj\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c39c16c0-190e-46c2-9023-20b65b8e3256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797/797 [==============================] - 1s 1ms/step\n",
      "Fold 10 Test RMSE: 0.01083408321326283\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test data for the current fold\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Fold {fold + 1} Test RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f7c75fba-07e6-495b-b7c2-70be02c3ff88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09519115,  0.25679642,  0.40343972,  0.50353152,  0.58671592,\n",
       "         0.67367211,  0.78347873,  0.92928667,  1.04266267,  1.08926999,\n",
       "         1.16278657,  1.24671786,  1.30415592,  1.89563547,  3.39994896,\n",
       "        11.0376358 ,  3.73687788,  3.29494602,  2.82269028,  2.51712663,\n",
       "         2.56750463,  2.85078022,  3.27094726,  3.41168626,  3.38808915,\n",
       "         3.29088464,  3.67159915,  5.06098318,  9.52640974, 35.07957198,\n",
       "        -0.25      , -0.5       ,  0.5       ]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform([X_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff888e-6a0c-4309-ad28-7501331df940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlpcw",
   "language": "python",
   "name": "nlpcw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
